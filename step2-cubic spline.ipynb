{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "# %matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "import io\n",
    "import time\n",
    "from datetime import datetime, timedelta, date\n",
    "import json\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "from scipy.interpolate import CubicSpline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#diable warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import North America data and split into Canada and US\n",
    "na_data = pd.read_csv(\"na_data.csv\")\n",
    "na_data['Time'] = pd.to_datetime(na_data['Time'])\n",
    "CND = na_data[(na_data[\"Country\"]==\"Canada\")]\n",
    "USA = na_data[(na_data[\"Country\"]==\"United States\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add time difference column\n",
    "CND['time_diff'] = CND['Time'].diff()\n",
    "USA['time_diff'] = USA['Time'].diff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_in_the_deep(dslic_interpolated):\n",
    "    '''\n",
    "    This function takes in a dataframe and returns a dataframe with rolling averages and rates\n",
    "    '''\n",
    "    all_data = dslic_interpolated\n",
    "    all_data = all_data.sort_values(by=['Level','Region', 'Country', 'State/Province','Time']).reset_index()\n",
    "    all_data = all_data.drop(columns=[\"index\"])\n",
    "    base_columns = [\n",
    "        'Level', 'Region','Country','Census Region','State/Province','Abbreviation','FIPS',\n",
    "        'Time', 'Date', 'Status',\n",
    "        'Cases Daily', 'Total Cases',\n",
    "        'Deaths Daily', 'Total Deaths',\n",
    "        'Tests Daily', 'Total Tests',\n",
    "        'Population', 'Population 100K',\n",
    "        'Country Population', 'Country Population 100K',\"Country Share\",\n",
    "        'Region Population', 'Region Population 100K',\"Region Share\",\n",
    "        'World Population', 'World Population 100K',\"World Share\",\n",
    "        'Accessed']\n",
    "    all_data = all_data[base_columns]\n",
    "    all_data['Time'] = all_data['Time'].astype(str)\n",
    "    all_min = all_data[\"Time\"].min()\n",
    "    all_max = all_data[\"Time\"].max()\n",
    "    all_data.head()\n",
    "    #print(all_data['State/Province'].unique())\n",
    "\n",
    "    grouping_cols = [\"Level\",\"Region\",\"Country\",\"State/Province\"]\n",
    "    base_cols = [\"Cases\",\"Tests\",\"Deaths\"]\n",
    "    calc_cols = []\n",
    "    for col in base_cols:\n",
    "        all_data[col + \" Daily\"] = all_data[col + \" Daily\"].fillna(0).astype(int)\n",
    "        all_data[col + \" Weekly\"] = all_data[col + \" Daily\"].rolling(7,min_periods=7).sum().reset_index(drop=True)\n",
    "        all_data[col + \" Daily Rate\"] = all_data[col + \" Daily\"]/all_data[\"Population 100K\"]\n",
    "        all_data[col + \" Weekly Rate\"] = all_data[col + \" Weekly\"]/all_data[\"Population 100K\"]\n",
    "        all_data[col + \" Total\"] = all_data.groupby(grouping_cols)[col + \" Daily\"].cumsum().reset_index(drop=True)\n",
    "        rolling_col = all_data.groupby(grouping_cols, as_index=False)[col + \" Daily\"].rolling(7,min_periods=7).mean().reset_index(drop=True)\n",
    "        all_data[col + \" Daily 7D Rolling\"] = rolling_col[col + \" Daily\"]    \n",
    "        all_data[col + \" Daily Rate 7D Rolling\"] = all_data.groupby(grouping_cols, as_index=False)[\"Cases Daily Rate\"].rolling(7,min_periods=7).mean().reset_index(drop=True)[\"Cases Daily Rate\"]\n",
    "        all_data[\"Total \" + col + \" Rate\"] = all_data[col + \" Total\"]/all_data[\"Population 100K\"]\n",
    "        base_order = [col + ' Daily', col + ' Daily 7D Rolling']\n",
    "        if col!=\"Vaccinations\":\n",
    "            base_order += [\"Total \" + col]\n",
    "        base_order += [col + \" Total\", col + ' Daily Rate', col + ' Daily Rate 7D Rolling', \"Total \" + col + \" Rate\"]\n",
    "        calc_cols = calc_cols + base_order\n",
    "\n",
    "    all_data[\"Speed Daily\"] = all_data[\"Cases Daily 7D Rolling\"]/all_data[\"Population 100K\"]\n",
    "    all_data[\"Speed Weekly\"] = all_data[\"Cases Weekly\"]/all_data[\"Population 100K\"]\n",
    "    all_data[\"Positivity 7D Rolling\"] = all_data[\"Cases Daily 7D Rolling\"]/all_data[\"Tests Daily 7D Rolling\"]\n",
    "    all_data[\"Positivity 7D Rolling\"] = all_data[\"Positivity 7D Rolling\"].apply(lambda x: np.nan if x == np.inf else x)\n",
    "    all_data[\"Positivity Weekly\"] = all_data[\"Cases Weekly\"]/all_data[\"Tests Weekly\"]\n",
    "    all_data[\"Positivity Weekly\"] = all_data[\"Positivity Weekly\"].apply(lambda x: np.nan if x == np.inf else x)\n",
    "    all_data[\"Acceleration Daily\"] = all_data.groupby(grouping_cols, as_index=False)[\"Speed Daily\"].diff().reset_index(drop=True)\n",
    "    all_data[\"Acceleration Weekly\"] = all_data.groupby(grouping_cols, as_index=False)[\"Speed Weekly\"].diff(7).reset_index(drop=True)\n",
    "    all_data[\"Jerk Daily\"] = all_data.groupby(grouping_cols, as_index=False)[\"Acceleration Daily\"].diff().reset_index(drop=True)\n",
    "    all_data[\"Jerk Weekly\"] = all_data.groupby(grouping_cols, as_index=False)[\"Acceleration Weekly\"].diff(7).reset_index(drop=True)\n",
    "    all_data[\"Jounce Daily\"] = all_data.groupby(grouping_cols, as_index=False)[\"Jerk Daily\"].diff().reset_index(drop=True)\n",
    "    all_data[\"MM-DD-YYYY\"] = all_data[\"Date\"].apply(lambda x: datetime.strptime(x, '%m/%d/%Y').strftime('%m-%d-%Y'))\n",
    "    all_data[\"MM-DD-YYYY\"] = all_data[\"MM-DD-YYYY\"].astype(str)\n",
    "    all_data[\"DD-MM-YYYY\"] = all_data[\"Date\"].apply(lambda x: datetime.strptime(x, '%m/%d/%Y').strftime('%d-%m-%Y'))\n",
    "    all_data[\"DD-MM-YYYY\"] = all_data[\"DD-MM-YYYY\"].astype(str)\n",
    "    all_data[\"Week\"] = all_data[\"Date\"].apply(\n",
    "        lambda x:\n",
    "        str(datetime.strptime(x, '%m/%d/%Y').isocalendar()[0]) +\n",
    "        \" W\" + str(datetime.strptime(x, '%m/%d/%Y').isocalendar()[1])\n",
    "    )\n",
    "    all_data[\"First Day of Week\"] = all_data[\"Date\"].apply(\n",
    "        lambda x: datetime.strptime(x, '%m/%d/%Y') - timedelta(days=datetime.strptime(x, '%m/%d/%Y').weekday())\n",
    "    )\n",
    "    all_data[\"Last Day of Week\"] = all_data[\"First Day of Week\"].apply(lambda x: x + timedelta(days=6))\n",
    "    def shortDate(day):\n",
    "        shortMonth = datetime.strftime(day,'%m').lstrip('0')\n",
    "        shortDay  = datetime.strftime(day,'%d').lstrip('0')\n",
    "        shortYear = datetime.strftime(day,'%y')\n",
    "        return shortMonth + \"/\" + shortDay + \"/\" + shortYear\n",
    "    all_data[\"Week Date Range\"] = all_data.apply(\n",
    "        lambda x: shortDate(x[\"First Day of Week\"]) + \" - \" + shortDate(x[\"Last Day of Week\"]),\n",
    "        axis=1\n",
    "    )\n",
    "    all_data[\"Day Count\"] = all_data.groupby([\"Region\",\"Country\",\"State/Province\",\"Date\"]).cumcount()\n",
    "\n",
    "    header_cols = [\n",
    "        'Level', 'Region','Country','Census Region','State/Province','Abbreviation','FIPS',\n",
    "        'Time', 'Date', 'Day Count', 'MM-DD-YYYY', 'DD-MM-YYYY', 'Week', 'First Day of Week', 'Last Day of Week', 'Week Date Range',\n",
    "        'Status'\n",
    "    ]\n",
    "    calc_cols = calc_cols + [\n",
    "        \"Positivity 7D Rolling\", \"Positivity Weekly\",\n",
    "        \"Speed Daily\", \"Speed Weekly\",\n",
    "        \"Acceleration Daily\",\"Acceleration Weekly\",\n",
    "        \"Jerk Daily\", \"Jerk Weekly\"\n",
    "    ]\n",
    "    other_cols = [\n",
    "        #'People Vaccinated', 'People Fully Vaccinated',\n",
    "    #    'Recovered Daily', 'Total Recovered',\n",
    "    #    'Active Daily', 'Total Active',\n",
    "        'Population', 'Population 100K',\n",
    "        'Country Population', 'Country Population 100K',\"Country Share\",\n",
    "        'Region Population', 'Region Population 100K',\"Region Share\",\n",
    "        'World Population', 'World Population 100K',\"World Share\",\n",
    "        'Accessed'#,'Stringency'\n",
    "    ]\n",
    "    all_cols = header_cols + calc_cols + other_cols\n",
    "    all_data = all_data[all_cols].copy()\n",
    "    last_monday = date(year=2021,month=1,day=18)\n",
    "    all_min = all_data[\"Time\"].min()\n",
    "    all_max = all_data[\"Time\"].max()\n",
    "    all_data.head()\n",
    "    return(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the version that deaths daily are not divided by 7,\n",
    "#the next block is the version that deaths daily are divided by 7,\n",
    "#use accordingly\n",
    "cnd_new = pd.DataFrame()\n",
    "\n",
    "takina = ['Alberta', 'British Columbia', 'Manitoba', 'New Brunswick',\n",
    "       'Newfoundland and Labrador', 'Northwest Territories',\n",
    "       'Nova Scotia', 'Nunavut', 'Ontario', 'Prince Edward Island',\n",
    "       'Quebec', 'Repatriated Travellers', 'Saskatchewan', 'Yukon']\n",
    "\n",
    "for pro in takina:\n",
    "    # get the daily data for the country\n",
    "    prod = CND[CND['State/Province'] == pro]\n",
    "    prod['Cases Daily'] = round(prod['Cases Daily']/7)\n",
    "    #prod['Deaths Daily'] = round(prod['Deaths Daily']/7)\n",
    "    prod['Tests Daily'] = round(prod['Tests Daily']/7)\n",
    "    \n",
    "    prod = prod.reset_index()\n",
    "    prod = prod.set_index('Time')  # set new index to 'Time' column\n",
    "    \n",
    "    dslic = prod.resample('D').ffill()\n",
    "    dslic['orderc'] = dslic['Cases Daily'].rolling(window=7, min_periods=1).mean()\n",
    "    dslic['orderd'] = dslic['Deaths Daily'].rolling(window=7, min_periods=1).mean()\n",
    "    dslic['ordert'] = dslic['Tests Daily'].rolling(window=7, min_periods=1).mean()    \n",
    "    print(dslic['State/Province'].unique())\n",
    "    \n",
    "    csc = CubicSpline(dslic.index.astype(int), dslic['orderc'])\n",
    "    csd = CubicSpline(dslic.index.astype(int), dslic['orderd'])\n",
    "    cst = CubicSpline(dslic.index.astype(int), dslic['ordert'])\n",
    "    \n",
    "    # generate new x values for the spline\n",
    "    new_dates = pd.date_range(dslic.index.min(), dslic.index.max(), freq='D')\n",
    "    \n",
    "    # evaluate the spline at the new x values\n",
    "    dslic_interpolated = dslic.reindex(new_dates)\n",
    "    dslic_interpolated['Cases Daily'] = csc(new_dates.astype(int))\n",
    "    dslic_interpolated['Deaths Daily'] = csd(new_dates.astype(int))\n",
    "    dslic_interpolated['Tests Daily'] = cst(new_dates.astype(int))\n",
    "    \n",
    "    dslic_interpolated = dslic_interpolated.reset_index()\n",
    "    dslic_interpolated = dslic_interpolated.rename(columns={'level_0': 'Time'})\n",
    "      \n",
    "    all_data = rolling_in_the_deep(dslic_interpolated)\n",
    "    cnd_new = cnd_new.append(all_data, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnd_new.to_csv(\"cnd_new_fx_0516.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the version that deaths daily are divided by 7\n",
    "cnd_new = pd.DataFrame()\n",
    "\n",
    "takina = ['Alberta', 'British Columbia', 'Manitoba', 'New Brunswick',\n",
    "       'Newfoundland and Labrador', 'Northwest Territories',\n",
    "       'Nova Scotia', 'Nunavut', 'Ontario', 'Prince Edward Island',\n",
    "       'Quebec', 'Repatriated Travellers', 'Saskatchewan', 'Yukon']\n",
    "\n",
    "for pro in takina:\n",
    "    # get the daily data for the country\n",
    "    prod = CND[CND['State/Province'] == pro]\n",
    "    prod['Cases Daily'] = round(prod['Cases Daily']/7)\n",
    "    prod['Deaths Daily'] = round(prod['Deaths Daily']/7)\n",
    "    prod['Tests Daily'] = round(prod['Tests Daily']/7)\n",
    "    \n",
    "    prod = prod.reset_index()\n",
    "    prod = prod.set_index('Time')  # set new index to 'Time' column\n",
    "    \n",
    "    dslic = prod.resample('D').ffill()\n",
    "    dslic['orderc'] = dslic['Cases Daily'].rolling(window=7, min_periods=1).mean()\n",
    "    dslic['orderd'] = dslic['Deaths Daily'].rolling(window=7, min_periods=1).mean()\n",
    "    dslic['ordert'] = dslic['Tests Daily'].rolling(window=7, min_periods=1).mean()    \n",
    "    print(dslic['State/Province'].unique())\n",
    "    \n",
    "    csc = CubicSpline(dslic.index.astype(int), dslic['orderc'])\n",
    "    csd = CubicSpline(dslic.index.astype(int), dslic['orderd'])\n",
    "    cst = CubicSpline(dslic.index.astype(int), dslic['ordert'])\n",
    "    \n",
    "    # generate new x values for the spline\n",
    "    new_dates = pd.date_range(dslic.index.min(), dslic.index.max(), freq='D')\n",
    "    \n",
    "    # evaluate the spline at the new x values\n",
    "    dslic_interpolated = dslic.reindex(new_dates)\n",
    "    dslic_interpolated['Cases Daily'] = csc(new_dates.astype(int))\n",
    "    dslic_interpolated['Deaths Daily'] = csd(new_dates.astype(int))\n",
    "    dslic_interpolated['Tests Daily'] = cst(new_dates.astype(int))\n",
    "    \n",
    "    dslic_interpolated = dslic_interpolated.reset_index()\n",
    "    dslic_interpolated = dslic_interpolated.rename(columns={'level_0': 'Time'})\n",
    "      \n",
    "    all_data = rolling_in_the_deep(dslic_interpolated)\n",
    "    cnd_new = cnd_new.append(all_data, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnd_new.to_csv(\"cnd_new_d7_fx_0516.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we calculate the usa data, also using cubic splines\n",
    "#USA data switched to weekly since October, 2022\n",
    "#you can see the state names to make sure your code is running\n",
    "usa_new = pd.DataFrame()\n",
    "\n",
    "for pro in USA['State/Province'].unique():\n",
    "    # get the daily data for the country\n",
    "    slic = USA[USA['State/Province'] == pro]\n",
    "    slic_1 = pd.DataFrame()\n",
    "    slic_not_1 = pd.DataFrame()\n",
    "\n",
    "    # Loop through each row of the original dataframe\n",
    "    for index, row in slic.iterrows():\n",
    "        my_string = str(row[\"time_diff\"])[0]\n",
    "        numeric_string = ''.join([char for char in my_string if char.isdigit()])\n",
    "        if len(numeric_string) != 0:\n",
    "            my_int = int(numeric_string)\n",
    "        else:\n",
    "            my_int = 1\n",
    "\n",
    "        if my_int != 1 and my_int != 0:\n",
    "            # Add the row to the dataframe where my_int != 1\n",
    "            row['Cases Daily'] = round(row['Cases Daily']/my_int)\n",
    "            row['Deaths Daily'] = round(row['Deaths Daily']/my_int)\n",
    "            row['Tests Daily'] = round(row['Tests Daily']/my_int)\n",
    "            slic_not_1 = slic_not_1.append(row)\n",
    "        elif my_int == 0:\n",
    "            continue\n",
    "        else:\n",
    "            slic_1 = slic_1.append(row)\n",
    "    \n",
    "    slic_1_zx = rolling_in_the_deep(slic_1)\n",
    "    last_row = slic_1.tail(1).copy()\n",
    "    slic_not_1 = slic_not_1.reset_index(drop=True)\n",
    "    slic_not_1 = pd.concat([last_row, slic_not_1], ignore_index=True)\n",
    "    slic_not_1 = slic_not_1.reset_index()\n",
    "    slic_not_1 = slic_not_1.set_index('Time')\n",
    "    dslic = slic_not_1.resample('D').ffill()\n",
    "\n",
    "    dslic['orderc'] = dslic['Cases Daily'].rolling(window=7, min_periods=1).mean()\n",
    "    dslic['orderd'] = dslic['Deaths Daily'].rolling(window=7, min_periods=1).mean()\n",
    "    dslic['ordert'] = dslic['Tests Daily'].rolling(window=7, min_periods=1).mean()    \n",
    "    print(dslic['State/Province'].unique())\n",
    "    \n",
    "    csc = CubicSpline(dslic.index.astype(int), dslic['orderc'])\n",
    "    csd = CubicSpline(dslic.index.astype(int), dslic['orderd'])\n",
    "    cst = CubicSpline(dslic.index.astype(int), dslic['ordert'])\n",
    "    \n",
    "    # generate new x values for the spline\n",
    "    new_dates = pd.date_range(dslic.index.min(), dslic.index.max(), freq='D')\n",
    "    \n",
    "    # evaluate the spline at the new x values\n",
    "    dslic_interpolated = dslic.reindex(new_dates)\n",
    "    dslic_interpolated['Cases Daily'] = csc(new_dates.astype(int))\n",
    "    dslic_interpolated['Deaths Daily'] = csd(new_dates.astype(int))\n",
    "    dslic_interpolated['Tests Daily'] = cst(new_dates.astype(int))\n",
    "    \n",
    "    dslic_interpolated = dslic_interpolated.reset_index()\n",
    "    dslic_interpolated = dslic_interpolated.rename(columns={'level_0': 'Time'})\n",
    "    \n",
    "    slic_not_1_zx = rolling_in_the_deep(dslic_interpolated)\n",
    "    slic_not_1_zx = slic_not_1_zx.drop(slic_not_1_zx.index[0])\n",
    "\n",
    "    state_d = pd.concat([slic_1_zx, slic_not_1_zx], ignore_index=True)\n",
    "    usa_new = usa_new.append(state_d, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usa_new.to_csv(\"usa_new_fx_0516.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you have the data ready to be used in step 3, below is a piece of viz code to help you understand the data better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmd = usa_new[usa_new['State/Province'] == 'New Mexico']\n",
    "fig, ax = plt.subplots(figsize=(15, 6))\n",
    "ax.plot(nmd['Time'], nmd['Cases Daily Rate 7D Rolling'])\n",
    "\n",
    "# set the size of the plot\n",
    "\n",
    "# add axis labels and title\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Cases Daily 7')\n",
    "plt.title('Daily Cases (7-day rolling average)')\n",
    "\n",
    "# display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
